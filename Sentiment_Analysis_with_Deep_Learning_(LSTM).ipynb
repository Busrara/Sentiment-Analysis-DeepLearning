{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### üõ†Ô∏è Step 1: Install & Import Required Libraries\n",
        "\n",
        "# Google Colab Users must first install the  install required libraries using:\n",
        "\n",
        "!pip install numpy pandas tensorflow keras scikit-learn spacy matplotlib\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Now, we import the necessary libraries:\n",
        "import numpy as np  # numerical computations\n",
        "import pandas as pd  # #Ô∏èData handling\n",
        "import spacy  # NLP processing (tokenization, lemmatization)\n",
        "import string  # String operations\n",
        "from sklearn.model_selection import train_test_split  # Splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Converting text to numbers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Standardizing input size\n",
        "from tensorflow.keras.utils import to_categorical  # One-hot encoding labels\n",
        "import matplotlib.pyplot as plt  # Visualization\n",
        "\n",
        "###üìå Step 2: Load and Clean the Data\n",
        "\n",
        "# Dataset Information:\n",
        "# Reviews are taken from Yelp, Amazon, and IMDB datasets.\n",
        "# Labels: 0 (Negative), 1 (Positive).\n",
        "# We must apply text cleaning (lowercasing, punctuation removal, and lemmatization to get rid of noise).\n",
        "\n",
        "# Load spaCy's English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Essential for tokenization & lemmatization\n",
        "\n",
        "# Write the function to load datasets\n",
        "def load_data():\n",
        "    column_name = ['Review', 'Sentiment']\n",
        "\n",
        "    # Read datasets from text files\n",
        "    data_yelp = pd.read_csv(\"yelp_labelled.txt\", sep='\\t', header=None, names=column_name)\n",
        "    data_amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep='\\t', header=None, names=column_name)\n",
        "    data_imdb = pd.read_csv(\"imdb_labelled.txt\", sep='\\t', header=None, names=column_name)\n",
        "\n",
        "    return pd.concat([data_yelp, data_amazon, data_imdb], ignore_index=True)\n",
        "\n",
        "# Now load data\n",
        "data = load_data()\n",
        "print(\"Dataset Loaded Successfully! Here is the shape:\", data.shape)\n",
        "\n",
        "# Write the function for getting rid of punctuation and do lemmatization\n",
        "def clean_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_.lower().strip() for token in doc if token.text not in string.punctuation]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Text cleaning command\n",
        "data[\"Cleaned_Review\"] = data[\"Review\"].apply(clean_text)\n",
        "\n",
        "# Convert labels (0: Negative, 1: Positive)\n",
        "data[\"Sentiment\"] = data[\"Sentiment\"].astype(int)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"Cleaned_Review\"], data[\"Sentiment\"], test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data Split Completed!\")\n",
        "\n",
        "###üìå Step 3: Tokenization & Padding\n",
        "# Why Tokenization?\n",
        "# - Convert words into numerical sequences.\n",
        "# - Use padding to ensure all sequences have the same length.\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")  # We limit vocab size to 5000 words\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Find max sequence length for padding\n",
        "max_length = max(len(seq) for seq in X_train_seq)\n",
        "\n",
        "# Apply padding\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding=\"post\")\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "print(\" Tokenization and Padding Finished!\")\n",
        "\n",
        "###üìå Step 4: Build the Deep Learning Model (LSTM)\n",
        "# Why LSTM?\n",
        "# It handles sequential data better than standard neural networks.\n",
        "# It also stores context from previous words.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=max_length),  # Word embeddings\n",
        "    LSTM(64, return_sequences=True),  # First LSTM layer\n",
        "    LSTM(32),  # Second LSTM layer\n",
        "    Dense(32, activation=\"relu\"),  # Fully connected layer\n",
        "    Dense(2, activation=\"softmax\")  # Output layer\n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "###üìå Step 5: Train the Model\n",
        "\n",
        "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "###üìå Step 6: Evaluate Model & Make Predictions\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
        "print(f\"test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# Predict on sample reviews\n",
        "sample_reviews = [\"This product is not useful.\", \"I loved it. It is very good.\"]\n",
        "sample_seq = tokenizer.texts_to_sequences(sample_reviews)\n",
        "sample_padded = pad_sequences(sample_seq, maxlen=max_length, padding=\"post\")\n",
        "\n",
        "predictions = model.predict(sample_padded)\n",
        "sentiments = [\"Negative\", \"Positive\"]\n",
        "\n",
        "for review, prediction in zip(sample_reviews, predictions):\n",
        "    print(f\"Review: \\\"{review}\\\" ‚Üí Sentiment: {sentiments[np.argmax(prediction)]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "EKylaUq-hqiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}